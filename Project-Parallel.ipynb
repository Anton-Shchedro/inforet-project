{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e61e96b-1de8-4bb6-8a65-edf8a14e5574",
   "metadata": {},
   "source": [
    "# Datasets preporation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df6daf-d18f-4fd5-ab97-2051be14c25b",
   "metadata": {},
   "source": [
    "__*This is a duplicate of a Project.ipyb, but with ipyparallel library for parallezation of jupyter notebook*__\n",
    "\n",
    "__Please adjust this code for your system.__\n",
    "\n",
    "(I use jupyter-lab with ipyparallel-labextension)\n",
    "\n",
    "__Please change *evaluation* function with correct path to this project__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871d3c2b-7724-463a-ac20-ee86a9d452be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff640e-2fbd-4f7a-8ed9-ccbb6be66425",
   "metadata": {},
   "source": [
    "Run next cell (rc.ids) untill it returns a list of cores (engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b093d6b6-6382-4ba2-a5ed-58672b5c06f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2641f9-f728-4b4d-9673-73eee4a8e417",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The zip file contains a large json file structured as follows. There is a 'segments' key that contains a list of speeches from members of parliament from 1948 to 2020. There is a lot of meta-data associated with each speech.. For the project, all you need is the field 'text'. I suggest you split the file into multiple files one for each legislature. Consider only the speeches with the field 'score' greater than 2.5 (the other speeches are meaningless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b492b3d-ddc0-424d-97fd-3dd949fcd325",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6c948-5120-4b8a-9cf0-09ed2932c3ba",
   "metadata": {},
   "source": [
    "## Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9842198e-19a6-4608-914f-1971c31b8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../speeches.json', \"r\") as read_file:\n",
    "    j = json.load(read_file)\n",
    "    \n",
    "text = pd.json_normalize(j, record_path =['segments'])\n",
    "text = text[text['score']>2.5]\n",
    "text = text.reset_index(drop=True)\n",
    "#dataset: text\n",
    "del j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48586505-9644-4ca6-9196-7c97a9da1aab",
   "metadata": {},
   "source": [
    "## Person Dataset (Info about deputy (Legislator) (dfp), senators (dfs) and ministers (dfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f19273-58b9-4c90-95b5-d34c311816de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load info about deputy\n",
    "df1 = pd.read_csv('parlamentari0x.csv')\n",
    "df2 = pd.read_csv('parlamentari1x.csv')\n",
    "dfp = pd.concat([df1, df2], ignore_index=True)\n",
    "# transform date from int to date format\n",
    "dfp['inizioMandato'] = pd.to_datetime(dfp['inizioMandato'], format=\"%Y%m%d\")\n",
    "dfp['fineMandato'] = dfp['fineMandato'].fillna(20990101)  # if person is currently active use placeholder as end date \n",
    "dfp['fineMandato'] = pd.to_datetime(dfp['fineMandato'], format=\"%Y%m%d\")\n",
    "# loan info about senators\n",
    "df1 = pd.read_csv('senato0x.csv')\n",
    "df2 = pd.read_csv('senato1x.csv')\n",
    "dfs = pd.concat([df1, df2], ignore_index=True)\n",
    "dfs['inizioMandato'] = pd.to_datetime(dfs['inizioMandato'], format=\"%Y%m%d\")\n",
    "dfs['fineMandato'] = dfs['fineMandato'].fillna(20990101)  # if person is currently active use placeholder as end date \n",
    "dfs['fineMandato'] = pd.to_datetime(dfs['fineMandato'], format=\"%Y%m%d\")\n",
    "# load info about ministers\n",
    "df1 = pd.read_csv('ministri0x.csv')\n",
    "df2 = pd.read_csv('ministri1x.csv')\n",
    "dfm = pd.concat([df1, df2], ignore_index=True)\n",
    "# rename some colums (to be abble to concat)\n",
    "dfm = dfm.rename(columns={\"d\": \"persona\"})\n",
    "dfp = dfp.rename(columns={\"inizioMandato\": \"dataInizio\", \"fineMandato\": \"dataFine\"})\n",
    "dfs = dfs.rename(columns={\"inizioMandato\": \"dataInizio\", \"fineMandato\": \"dataFine\"})\n",
    "# replace ministers link to it's personal link (have same numerical number, need only to change prefix and remove '_xx')\n",
    "for i, d in dfm.iterrows():\n",
    "    m = re.search('deputato.rdf/d(.+?)_', d['persona'])\n",
    "    dfm.at[i,'persona'] = 'http://dati.camera.it/ocd/persona.rdf/p'+m.group(1)\n",
    "# transform date from int to date format   \n",
    "dfm['dataInizio'] = pd.to_datetime(dfm['dataInizio'], format=\"%Y%m%d\")\n",
    "dfm['dataFine'] = dfm['dataFine'].fillna(20990101)\n",
    "dfm['dataFine'] = pd.to_datetime(dfm['dataFine'], format=\"%Y%m%d\")\n",
    "# need only some colums\n",
    "dfp = dfp[['persona','cognome','nome','dataInizio','dataFine']]\n",
    "dfm = dfm[['persona','cognome','nome','dataInizio','dataFine','carica','nomeOrganoGoverno']]\n",
    "# final concat into one df\n",
    "filtered_personnel = pd.concat([dfp, dfm, dfs], ignore_index=True)\n",
    "filtered_personnel = filtered_personnel.rename(columns={\"cognome\": \"surname\", \"nome\": \"name\"})\n",
    "filtered_personnel =  filtered_personnel[['persona','surname','name','dataInizio','dataFine','carica','nomeOrganoGoverno']]\n",
    "# dataset: filtered_personnel\n",
    "del df1, df2, dfp, dfs, dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b8d7a-31b2-4c04-971b-bdaae132b371",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make a sample test dataset\n",
    "\n",
    "Just to make it easier to work or test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918541dd-1da4-41a8-840e-1867f5f17786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_df = np.array_split(text, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844fed4-8513-4e3e-924d-7f1cb64a0c27",
   "metadata": {},
   "source": [
    "In following I use a split_df that still contains whole text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e191ea-eab3-4f0d-8ddb-0dae318309b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# First Method\n",
    "\n",
    "via surname == (words[0]+' '+words[1]) and other combinations\n",
    "\n",
    "Fast enough without parallization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029b70e-f72e-4172-8eca-a4dc8f19791b",
   "metadata": {},
   "source": [
    "Word correction by Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d8f21c-dd3f-4ba5-b164-689ce8e67666",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SURNAME done\n",
      "NAME done\n",
      "TEXT done\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def words(text): \n",
    "    text = text.upper().replace(\"À\",\"A'\").replace(\"È\",\"E'\").replace(\"É\",\"E'\").replace(\"Ì\",\"I'\").replace(\"Í\",\"I'\").replace(\"Ò\",\"O'\").replace(\"Ó\",\"O'\").replace(\"Ù\",\"U'\").replace(\"Ú\",\"U'\").replace(\"Ü\",\"U'\")\n",
    "    return re.findall(r\"[A-Z'-]+\", text.upper())\n",
    "SURNAME = Counter(words(' '.join(filtered_personnel['surname'])))\n",
    "print(\"SURNAME done\")\n",
    "NAME = Counter(words(' '.join(filtered_personnel['name'])))\n",
    "print(\"NAME done\")\n",
    "t = ' '.join(text['text'])\n",
    "\n",
    "TEXT = Counter(words(t))\n",
    "print(\"TEXT done\")\n",
    "WORDS = SURNAME\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a66b0b-f9e0-4e38-a7cd-b8270377baa7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = \"abcdefghijklmnopqrstuvwxyz'\".upper()\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832d784-4218-4322-937c-3f30545b634a",
   "metadata": {},
   "source": [
    "NER is made by find_person function.\n",
    "\n",
    "1) Transform text in acceptable form:\n",
    "\n",
    "    a) replace diacritics with \" ' \" symbol\n",
    "    \n",
    "    b) extract first 3 words. Not alphabetic words in beggining of text is skiped until first alphabetic word is found.\n",
    "    \n",
    "2) Ceate list of persons based on a date of text\n",
    "3) Reduce list (2) with find_by_surname function.\n",
    "4) If list (3) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "5) If list (3) have more than 1 id, reduce this list with find_by_name function.\n",
    "6) If list (5) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "7) If list still have more than 1 id, or list is empty, return None (in paper was shown as 'no person found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f3d826-96b8-43db-8530-77af185aae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_by_surname(words,personnel, corr):\n",
    "    if len(words) > 2:\n",
    "        sur = personnel[(personnel[\"surname\"] == words[0].upper()) #first word is surname\n",
    "                       | (personnel[\"surname\"] == words[1].upper()) #second word is surname\n",
    "                       | (personnel[\"surname\"] == words[2].upper()) #third word is surname\n",
    "                       | (personnel[\"surname\"] == (words[0]+' '+words[1]).upper()) #surname by multiple words\n",
    "                       | (personnel[\"surname\"] == (words[1]+' '+words[2]).upper())] #surname by multiple words\n",
    "    elif len(words) == 2:\n",
    "        sur = personnel[(personnel[\"surname\"] == words[0].upper()) #first word is surname\n",
    "                       | (personnel[\"surname\"] == words[1].upper()) #second word is surname\n",
    "                       | (personnel[\"surname\"] == (words[0]+' '+words[1]).upper())] #surname by multiple words\n",
    "    else:\n",
    "        sur = personnel[(personnel[\"surname\"] == words[0].upper())] #first word is surname\n",
    "    if len(sur.index) == 0 and not corr:\n",
    "        corr = True\n",
    "        WORDS = SURNAME\n",
    "        words[0] = correction(words[0].upper())\n",
    "        if len(words) == 2:\n",
    "            words[1] = correction(words[1].upper())\n",
    "        elif len(words) > 2:\n",
    "            words[2] = correction(words[2].upper())\n",
    "        sur = find_by_surname(words,personnel,corr)\n",
    "    return sur\n",
    "\n",
    "def find_by_name(words,personnel,corr):\n",
    "    if len(words) > 2:\n",
    "        sur = personnel[(personnel[\"name\"] == words[0].upper()) #first word is name\n",
    "                       | (personnel[\"name\"] == words[1].upper()) #second word is name\n",
    "                       | (personnel[\"name\"] == words[2].upper()) #third word is name\n",
    "                       | (personnel[\"name\"] == (words[0]+' '+words[1]).upper()) #name by multiple words\n",
    "                       | (personnel[\"name\"] == (words[1]+' '+words[2]).upper())] #name by multiple words\n",
    "    elif len(words) == 2:\n",
    "        sur = personnel[(personnel[\"name\"] == words[0].upper()) #first word is name\n",
    "                       | (personnel[\"name\"] == words[1].upper()) #second word is name\n",
    "                       | (personnel[\"name\"] == (words[0]+' '+words[1]).upper())] #name by multiple words\n",
    "    else:\n",
    "        sur = personnel[(personnel[\"name\"] == words[0].upper())] #first word is name\n",
    "    if len(pd.unique(sur[\"name\"])) > 1 and len(words) > 1: # if more then one surename in result, reduce up to first in text\n",
    "        sur = find_by_name(words[0:-1],sur, False)\n",
    "    if len(sur.index) == 0 and not corr:\n",
    "        corr = True\n",
    "        WORDS = NAME\n",
    "        words[0] = correction(words[0].upper())\n",
    "        if len(words) == 2:\n",
    "            words[1] = correction(words[1].upper())\n",
    "        elif len(words) > 2:\n",
    "            words[2] = correction(words[2].upper())\n",
    "        sur = find_by_surname(words,personnel,corr)\n",
    "    return sur\n",
    "\n",
    "def find_person(text, date, personnel):\n",
    "    global phase  # used for debug and error log\n",
    "    global l # size of text dataset\n",
    "    phase = \"char_replace\"\n",
    "    text = text.replace(\"À\",\"A'\")\n",
    "    text = text.replace(\"È\",\"E'\")\n",
    "    text = text.replace(\"É\",\"E'\")\n",
    "    text = text.replace(\"Ì\",\"I'\")\n",
    "    text = text.replace(\"Ò\",\"O'\")\n",
    "    text = text.replace(\"Ó\",\"O'\")\n",
    "    text = text.replace(\"Ù\",\"U'\")\n",
    "    phase = \"words\"\n",
    "    words = regex.sub('', text).split(maxsplit = 4)\n",
    "    if len(words)==0:   # text is empty\n",
    "        l -= 1\n",
    "        return None\n",
    "    while (not words[0].isalpha()):  # use only words.\n",
    "        words.pop(0)\n",
    "        if len(words) == 0:\n",
    "            return None\n",
    "        t = ' '.join(words)    # this and next line is used just because i limit split up to 4\n",
    "        words = regex.sub('', t).split(maxsplit = 4)\n",
    "    phase = \"date_search\"\n",
    "    date = personnel[(personnel[\"dataInizio\"] <= date) & (personnel[\"dataFine\"] >= date)]  # start with persons in possitions in a date of speech\n",
    "    if len(date.index) >= 1:\n",
    "        phase = \"sur_search\"\n",
    "        sur = find_by_surname(words, date, False)\n",
    "        #print(sur)\n",
    "        if len(sur.index) == 1:\n",
    "            return sur.iloc[0]['persona']\n",
    "        elif len(sur.index) > 1:\n",
    "            if len(pd.unique(sur['persona'])) == 1:\n",
    "                return sur.iloc[0]['persona']\n",
    "            elif len(words) > 1:\n",
    "                phase = \"name_search\"\n",
    "                name = find_by_name(words, sur, False)\n",
    "                #print('-------------------')\n",
    "                #print(name)\n",
    "                if len(name.index) == 1 or (len(name.index) > 1 and len(pd.unique(name['persona']))):\n",
    "                    return name.iloc[0]['persona']\n",
    "    else:#No person found with that surename with correction\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7bf5b-1a1e-4f05-a489-8fb33e1a4d0e",
   "metadata": {},
   "source": [
    "For each text record find preson with find_person function. It returns a id of person (or None). \n",
    "\n",
    "Check if predicted id is same as in text redord (saved in bool variable \"Correct\").\n",
    "\n",
    "Append result to a list of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04daf91-c4b1-4343-acf9-7637a88c3c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     1000\n",
       "unique       2\n",
       "top       True\n",
       "freq       786\n",
       "Name: Correct, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(text.index)\n",
    "res = []\n",
    "errors = []\n",
    "\n",
    "regex = re.compile(\"[^a-zA-Z -']\")\n",
    "\n",
    "ind = 0\n",
    "\n",
    "for index, row in text2.iterrows():\n",
    "    global phase\n",
    "    try:\n",
    "        person = find_person(row['text'], row['date'], filtered_personnel)\n",
    "    except:\n",
    "        errors.append({'index':index, 'text':row['text'], 'phase':phase})\n",
    "        continue\n",
    "    predicted = filtered_personnel.loc[filtered_personnel['persona'] == person]\n",
    "    if person is not None:\n",
    "        pred_surname = predicted.iloc[0]['surname']\n",
    "        pred_name = predicted.iloc[0]['name']\n",
    "    else:\n",
    "        pred_surname = None\n",
    "        pred_name = None\n",
    "    if row[\"persona\"] == person:\n",
    "        correct = True\n",
    "    else:\n",
    "        correct = False\n",
    "    \n",
    "    res.append({\"True persona\": row[\"persona\"], \"True name\": row[\"name\"], \"True surname\": row[\"surname\"],\n",
    "                \"Text\":row[\"text\"],  \"Correct\":correct, \"Predicted persona\": person,\n",
    "                \"Predicted surname\": pred_surname, \n",
    "                \"Predicted name\": pred_name})\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Done record '+str(index))\n",
    "    ind += 1\n",
    "    print(str(ind)+'/'+str(l)+'    '+str((ind/l)*100)+'%')\n",
    "            \n",
    "clear_output(wait=True)\n",
    "print('Done')\n",
    "result = pd.DataFrame.from_dict(res)\n",
    "result['Correct'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff8676-4a44-4d31-babf-13bf27c2f097",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Second Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e93de-90bd-46d7-92c7-62ee6d48d81c",
   "metadata": {},
   "source": [
    "Word correction by Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b22fae60-f018-4356-a937-9382b1e0fb89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SURNAME done\n",
      "NAME done\n",
      "TEXT done\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def words(text): \n",
    "    text = text.upper().replace(\"À\",\"A'\").replace(\"È\",\"E'\").replace(\"É\",\"E'\").replace(\"Ì\",\"I'\").replace(\"Í\",\"I'\").replace(\"Ò\",\"O'\").replace(\"Ó\",\"O'\").replace(\"Ù\",\"U'\").replace(\"Ú\",\"U'\").replace(\"Ü\",\"U'\")\n",
    "    return re.findall(r\"[A-Z'-]+\", text.upper())\n",
    "SURNAME = Counter(words(' '.join(filtered_personnel['surname'])))\n",
    "print(\"SURNAME done\")\n",
    "NAME = Counter(words(' '.join(filtered_personnel['name'])))\n",
    "print(\"NAME done\")\n",
    "t = ' '.join(text['text'])\n",
    "\n",
    "TEXT = Counter(words(t))\n",
    "print(\"TEXT done\")\n",
    "WORDS = SURNAME\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c95608-62b3-4807-9cc2-1a6d330dd3da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = \"abcdefghijklmnopqrstuvwxyz'\".upper()\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb31ec-4cf8-4b88-b677-2d3c39f4b4bb",
   "metadata": {},
   "source": [
    "create one-gramms used in word-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a1b3e1a-195b-4bd5-9793-7329fa1d2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"one-grams.txt\", 'w') as f:\n",
    "    c = TEXT+SURNAME+NAME\n",
    "    for k,v in  c.most_common():\n",
    "        f.write( \"{}\\t{}\\n\".format(k.lower(),v) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d6bf0-96a1-43ce-84dc-62d529eb273e",
   "metadata": {},
   "source": [
    "NER is made by find_person function.\n",
    "\n",
    "1) Transform text in acceptable form:\n",
    "\n",
    "    a) replace diacritics with \" ' \" symbol\n",
    "    \n",
    "    b) calculate average lingth of words. If it's too small, probably exist a problem with extra spaces between letters. It can be fixed with word-segmentation (https://jeremykun.com/2012/01/15/word-segmentation/)\n",
    "    \n",
    "2) Ceate list of persons based on a date of text\n",
    "3) Reduce list (2) with find_by_surname function.\n",
    "4) If list (3) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "5) If list (3) have more than 1 id, reduce this list by finding first surname appeard in text.\n",
    "6) If list (5) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "7) If list (5) have more than 1 id, reduce this list with find_by_name function.\n",
    "8) If list (7) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "9) If list still have more than 1 id, or list is empty, return None (in paper was shown as 'no person found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cade5b41-79b9-4618-b6cb-3db926bb8ac6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_by_surname(text,personnel, corr):\n",
    "    import re\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    for i, s in personnel.iterrows():\n",
    "        if s['surname'] in text:\n",
    "            data.append(s)\n",
    "    sur = pd.DataFrame(data)\n",
    "    if len(sur.index) == 0 and not corr:\n",
    "        WORDS = SURNAME+TEXT\n",
    "        word = regex.sub('', text).split()\n",
    "        for i, w in enumerate(word):\n",
    "            word[i] = correction(w)\n",
    "        sur = find_by_surname(' '.join(word),personnel,True)\n",
    "    return sur\n",
    "\n",
    "def find_by_name(text,personnel,corr):\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    for i, s in personnel.iterrows():\n",
    "        if s['name'] in text:\n",
    "            data.append(s)\n",
    "    sur = pd.DataFrame(data)\n",
    "    return sur\n",
    "\n",
    "def find_person(text, date, personnel):\n",
    "    import re\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    import pandas as pd\n",
    "    import segment\n",
    "    global phase\n",
    "    global l\n",
    "    phase = \"char_replace\"\n",
    "    text = text.replace(\"À\",\"A'\")\n",
    "    text = text.replace(\"È\",\"E'\")\n",
    "    text = text.replace(\"É\",\"E'\")\n",
    "    text = text.replace(\"Ì\",\"I'\")\n",
    "    text = text.replace(\"Ò\",\"O'\")\n",
    "    text = text.replace(\"Ó\",\"O'\")\n",
    "    text = text.replace(\"Ù\",\"U'\")\n",
    "    \n",
    "    # take only up to 20 first words\n",
    "    word = regex.sub('', text).split()\n",
    "    average = sum(len(w) for w in word) / len(word)\n",
    "    if average < 4:\n",
    "        if len(word) >= 120: #avarage 6-8 letters in italian word (6*20 = 120), but segment have limit of 100\n",
    "            seg = segment.segment(''.join(word[0:60]))\n",
    "        else:\n",
    "            seg = segment.segment(''.join(word))\n",
    "        text = ' '.join(seg)\n",
    "    elif len(word) >= 20:\n",
    "        text = ' '.join(word[0:20])\n",
    "    else:\n",
    "        text = ' '.join(word)\n",
    "    #print(text)\n",
    "    phase = \"date_search\"\n",
    "    date = personnel[(personnel[\"dataInizio\"] <= date) & (personnel[\"dataFine\"] >= date)]\n",
    "    if len(date.index) >= 1:\n",
    "        phase = \"sur_search\"\n",
    "        sur = find_by_surname(text.upper(), date, False)\n",
    "        phase = \"after_sur_search\"\n",
    "        #print(sur)\n",
    "        if len(sur.index) == 0:\n",
    "            return None\n",
    "        elif (len(sur.index) == 1) or (len(pd.unique(sur['persona'])) == 1):\n",
    "            return sur.iloc[0]['persona']\n",
    "        else:\n",
    "            if len(sur['surname'].value_counts()) > 1:\n",
    "                word = regex.sub('', text.upper()).split()\n",
    "                sur2 = pd.DataFrame({'A' : []})\n",
    "                i = 0\n",
    "                t = ''\n",
    "                while(sur2.empty and (i < 20 and i < len(word))): #limit of frase up to 20 words\n",
    "                    t = t + ' ' + word[i]\n",
    "                    sur2 = find_by_surname(t,sur,False) # check what surname is appering first in text \n",
    "                    i+=1\n",
    "                sur = sur2\n",
    "                if sur.empty:\n",
    "                    return None\n",
    "            #print(sur)\n",
    "            if len(pd.unique(sur['persona'])) == 1:\n",
    "                return sur.iloc[0]['persona']\n",
    "            else:\n",
    "                phase = \"name_search\"\n",
    "                name = find_by_name(text.upper(), sur, False)\n",
    "                #print('-------------------')\n",
    "                #print(name)\n",
    "                if len(name.index) == 1 or (len(name.index) > 1 and len(pd.unique(name['persona']))==1):\n",
    "                    return name.iloc[0]['persona']\n",
    "    else:#No person found with that surename with correction\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98476b37-26de-4605-838c-a1c5dd0ba73a",
   "metadata": {},
   "source": [
    "For each text record find preson with find_person function. It returns a id of person (or None). \n",
    "\n",
    "Check if predicted id is same as in text redord (saved in bool variable \"Correct\").\n",
    "\n",
    "Append result to a list of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0124670-6e75-461d-8b3d-af3bd73bf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(row):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    import sys\n",
    "    sys.path.insert(0, 'Desktop/Unimi/information retriveal/project/corpus (Alfio)/inforet-project/')\n",
    "    try:\n",
    "        import os\n",
    "        os.chdir('Desktop/Unimi/information retriveal/project/corpus (Alfio)/inforet-project/')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    res = []\n",
    "    errors = []\n",
    "    \n",
    "    global regex\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "\n",
    "    global phase\n",
    "    try:\n",
    "        person = find_person(row['text'], row['date'], filtered_personnel)\n",
    "    except Exception as ex:\n",
    "        #errors.append({'index':index, 'text':row['text'], 'phase':phase, 'ex':ex})\n",
    "        errors.append({'text':row['text'], 'phase':phase, 'ex':ex})\n",
    "        return pd.DataFrame.from_dict(res), pd.DataFrame.from_dict(errors)\n",
    "        #continue\n",
    "    predicted = filtered_personnel.loc[filtered_personnel['persona'] == person]\n",
    "    if person is not None:\n",
    "        pred_surname = predicted.iloc[0]['surname']\n",
    "        pred_name = predicted.iloc[0]['name']\n",
    "    else:\n",
    "        pred_surname = None\n",
    "        pred_name = None\n",
    "    if row[\"persona\"] == person:\n",
    "        correct = True\n",
    "    else:\n",
    "        correct = False\n",
    "\n",
    "    res.append({\"True persona\": row[\"persona\"], \"True name\": row[\"name\"], \"True surname\": row[\"surname\"],\n",
    "                \"Text\":row[\"text\"],  \"Correct\":correct, \"Predicted persona\": person,\n",
    "                \"Predicted surname\": pred_surname, \n",
    "                \"Predicted name\": pred_name})\n",
    "\n",
    "    return pd.DataFrame.from_dict(res), pd.DataFrame.from_dict(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8515eab-b424-40d7-a2be-b62c63890d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc[:].push(dict(\n",
    "    words = words,\n",
    "    P = P,\n",
    "    correction = correction,\n",
    "    candidates = candidates,\n",
    "    known = known,\n",
    "    edits1 = edits1,\n",
    "    edits2 = edits2,\n",
    "    filtered_personnel=filtered_personnel,\n",
    "    find_person = find_person,\n",
    "    find_by_surname = find_by_surname,\n",
    "    find_by_name = find_by_name,\n",
    "    SURNAME = SURNAME,\n",
    "    NAME = NAME,\n",
    "    TEXT = TEXT,\n",
    "    WORDS = WORDS\n",
    "))\n",
    "view = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cf644c5-8f58-45d8-9977-1ec6d2334c00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text         phase  \\\n",
      "0  VALENSISE RAFFAELE (gruppo MSI-destra nazional...  char_replace   \n",
      "\n",
      "                                  ex  \n",
      "0  int too large to convert to float  \n",
      "Done: 0/1000\n",
      "Done: 1/1000\n",
      "CPU times: total: 42.2 s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, df in enumerate(split_df):\n",
    "    ar = view.map_async(evaluation, [row for index, row in df.iterrows()])\n",
    "    ar.wait() # Wait until all tasks are done.\n",
    "    results = pd.DataFrame({'A' : []})\n",
    "    for res in ar:\n",
    "        #if len(res[1])>0:      #uncomment this line to print a list of errors\n",
    "            #print(res[1])      #uncomment this line to print a list of errors\n",
    "        results = pd.concat([results, res[0]], ignore_index=True)\n",
    "    results = results.drop(columns=['A'])\n",
    "    file_name = './result/res'+str(i)+'.csv'\n",
    "    results.to_csv(file_name,index=False)\n",
    "    clear_output(wait=True)     #comment this line to print a list of errors (otherwise this (printed) output will be cleared)\n",
    "    print('Done: ' + str(i) + '/' + str(len(split_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf391a-b6b6-46a6-af5a-63a7c1cf4c79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Third Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5f28d-b807-4fbb-9de2-433a9c352374",
   "metadata": {},
   "source": [
    "Word correction by Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04656ac-010e-42db-a2e7-17aa5cfe8812",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SURNAME done\n",
      "NAME done\n",
      "TEXT done\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def words(text): \n",
    "    text = text.upper().replace(\"À\",\"A'\").replace(\"È\",\"E'\").replace(\"É\",\"E'\").replace(\"Ì\",\"I'\").replace(\"Í\",\"I'\").replace(\"Ò\",\"O'\").replace(\"Ó\",\"O'\").replace(\"Ù\",\"U'\").replace(\"Ú\",\"U'\").replace(\"Ü\",\"U'\")\n",
    "    return re.findall(r\"[A-Z'-]+\", text.upper())\n",
    "SURNAME = Counter(words(' '.join(filtered_personnel['surname'])))\n",
    "print(\"SURNAME done\")\n",
    "NAME = Counter(words(' '.join(filtered_personnel['name'])))\n",
    "print(\"NAME done\")\n",
    "t = ' '.join(text['text'])\n",
    "\n",
    "TEXT = Counter(words(t))\n",
    "print(\"TEXT done\")\n",
    "WORDS = SURNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ce9963-7681-4faa-9fad-257f9a85c495",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = \"abcdefghijklmnopqrstuvwxyz'\".upper()\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6059ce6-fda5-4dbc-82ea-d1faf31cd264",
   "metadata": {},
   "source": [
    "Create one-gramms used in word-segmentation and create list of surnames to use it in surname word-segmentation search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adfde28-11bc-4b6a-b31c-f551a2d0c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"one-grams.txt\", 'w') as f:\n",
    "    c = TEXT+SURNAME+NAME\n",
    "    for k,v in  c.most_common():\n",
    "        f.write( \"{}\\t{}\\n\".format(k.lower(),v) )\n",
    "s = filtered_personnel[\"surname\"].value_counts()\n",
    "with open(\"surname2-one-grams.txt\", 'w') as f:\n",
    "    for k,v in s.iteritems():\n",
    "        k = k.replace(\"À\",\"A'\").replace(\"È\",\"E'\").replace(\"É\",\"E'\").replace(\"Ì\",\"I'\").replace(\"Í\",\"I'\").replace(\"Ò\",\"O'\").replace(\"Ó\",\"O'\").replace(\"Ù\",\"U'\").replace(\"Ú\",\"U'\").replace(\"Ü\",\"U'\")\n",
    "        string = k.lower() + \"\\t\\t\" + str(v) + \"\\n\"\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b4b81-b5cf-40a2-9a27-9d464a361b58",
   "metadata": {},
   "source": [
    "NER is made by find_person function.\n",
    "\n",
    "1) Transform text in acceptable form:\n",
    "\n",
    "    a) replace diacritics with \" ' \" symbol\n",
    "    \n",
    "    b) calculate average lingth of words. If it's too small, probably exist a problem with extra spaces between letters. It can be fixed with word-segmentation (https://jeremykun.com/2012/01/15/word-segmentation/)\n",
    "    \n",
    "2) Ceate list of persons based on a date of text\n",
    "3) Reduce list (2) with find_by_surname_2 function.\n",
    "4) If list (3) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "5) If list (3) have more than 1 id, reduce this list with find_by_name function.\n",
    "6) If list (5) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "7) If list (5) have more than 1 id, reduce this list by finding first surname appeard in text.\n",
    "8) If list (7) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "9) If list (5) is empty, reduce list (3) by finding first surname appeard in text.\n",
    "10) If list (9) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "11) If list still have more than 1 id, or list is empty, return None (in paper was shown as 'no person found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437be32f-ce18-432d-bc71-662b1e4b0aff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_by_surname(text,personnel, corr):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    data = []\n",
    "    for i, s in personnel.iterrows():\n",
    "        if s['surname'] in text:\n",
    "            data.append(s)\n",
    "    sur = pd.DataFrame(data)\n",
    "    if len(sur.index) == 0 and not corr:\n",
    "        WORDS = SURNAME+TEXT\n",
    "        word = regex.sub('', text).split()\n",
    "        for i, w in enumerate(word):\n",
    "            word[i] = correction(w)\n",
    "        sur = find_by_surname(' '.join(word),personnel,True)\n",
    "    return sur\n",
    "\n",
    "def find_by_surname_2(text,personnel, corr):\n",
    "    global phase\n",
    "    import segment2_sur\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    data = []\n",
    "    phase = 'sur_segment'\n",
    "    seg = segment2_sur.segment(text)\n",
    "    sur = pd.DataFrame({'A' : []})\n",
    "    #print(seg)\n",
    "    phase = 'sur_loop'\n",
    "    for s in seg:\n",
    "        p = personnel[(personnel[\"surname\"] == s.upper())]\n",
    "        sur = pd.concat([sur,p])\n",
    "    phase = 'sur_loop_end'\n",
    "    if len(sur.index) == 0 and not corr:\n",
    "        phase = 'sur_corr'\n",
    "        WORDS = SURNAME+TEXT\n",
    "        word = regex.sub('', text).split()\n",
    "        for i, w in enumerate(word):\n",
    "            word[i] = correction(w)\n",
    "        sur = find_by_surname_2(' '.join(word),personnel,True)\n",
    "    return sur\n",
    "\n",
    "def find_by_name(text,personnel,corr):\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    for i, s in personnel.iterrows():\n",
    "        if s['name'] in text:\n",
    "            data.append(s)\n",
    "    sur = pd.DataFrame(data)\n",
    "    return sur\n",
    "\n",
    "def find_person(text, date, personnel):\n",
    "    global phase\n",
    "    global l\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import segment\n",
    "    phase = \"char_replace\"\n",
    "    text = text.upper().replace(\"À\",\"A'\")\n",
    "    text = text.replace(\"È\",\"E'\")\n",
    "    text = text.replace(\"É\",\"E'\")\n",
    "    text = text.replace(\"Ì\",\"I'\")\n",
    "    text = text.replace(\"Ò\",\"O'\")\n",
    "    text = text.replace(\"Ó\",\"O'\")\n",
    "    text = text.replace(\"Ù\",\"U'\")\n",
    "    \n",
    "    # take only up to 20 first words\n",
    "    phase = \"segment_start\"\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    word = regex.sub('', text).split()\n",
    "    average = sum(len(w) for w in word) / len(word)\n",
    "    if average < 4:\n",
    "        phase = \"segment\"\n",
    "        if len(word) >= 60: #avarage 6-8 letters in italian word (6*20 = 120), but segment have limit of 60\n",
    "            seg = segment.segment(''.join(word[0:60]))\n",
    "        else:\n",
    "            seg = segment.segment(''.join(word))\n",
    "        text = ' '.join(seg)\n",
    "    elif len(word) >= 20:\n",
    "        phase = \"segment2\"\n",
    "        text = ' '.join(word[0:20])\n",
    "    else:\n",
    "        text = ' '.join(word)\n",
    "    #print(text)\n",
    "    phase = \"date_search\"\n",
    "    date = personnel[(personnel[\"dataInizio\"] <= date) & (personnel[\"dataFine\"] >= date)]\n",
    "    if len(date.index) >= 1:\n",
    "        phase = \"sur_search\"\n",
    "        sur = find_by_surname_2(text.upper(), date, False)\n",
    "        phase = \"sur_search_end\"\n",
    "        #print(sur)\n",
    "        if len(sur.index) == 0:\n",
    "            return None\n",
    "        elif (len(sur.index) == 1) or (len(pd.unique(sur['persona'])) == 1):\n",
    "            return sur.iloc[0]['persona']\n",
    "        else:\n",
    "            phase = \"name_search\"\n",
    "            name = find_by_name(text.upper(), sur, False)\n",
    "            #print('-------------------')\n",
    "            #print(name)\n",
    "            if len(name.index) == 1 or (len(name.index) > 1 and len(pd.unique(name['persona']))==1):\n",
    "                return name.iloc[0]['persona']\n",
    "            elif (len(name.index) > 1 and len(name['surname'].value_counts()) > 1):\n",
    "                phase = \"sur_search_2\"\n",
    "                word = regex.sub('', text.upper()).split()\n",
    "                sur2 = pd.DataFrame({'A' : []})\n",
    "                i = 0\n",
    "                while(sur2.empty and (i < 20 and i < len(word))): #limit of frase up to 20 words\n",
    "                    sur2 = find_by_surname(' '.join(word[0:i]),name,False) # check what surname is appering first in text \n",
    "                    i+=1\n",
    "                sur = sur2\n",
    "                #print(\"----------\")\n",
    "                #print(sur)\n",
    "                if len(sur.index) == 0:\n",
    "                    return None\n",
    "                elif (len(sur.index) == 1) or (len(pd.unique(sur['persona'])) == 1):\n",
    "                    return sur.iloc[0]['persona']\n",
    "                else: return None\n",
    "            else: #difference between this else and elif is in dataframe used (in elif 'name', in else 'sur')\n",
    "                phase = \"sur_search_3\"\n",
    "                word = regex.sub('', text.upper()).split()\n",
    "                sur2 = pd.DataFrame({'A' : []})\n",
    "                i = 0\n",
    "                while(sur2.empty and (i < 20 and i < len(word))): #limit of frase up to 20 words\n",
    "                    sur2 = find_by_surname(' '.join(word[0:i]),sur,False) # check what surname is appering first in text \n",
    "                    i+=1\n",
    "                sur = sur2\n",
    "                #print(\"----------\")\n",
    "                #print(sur)\n",
    "                if len(sur.index) == 0:\n",
    "                    return None\n",
    "                elif (len(sur.index) == 1) or (len(pd.unique(sur['persona'])) == 1):\n",
    "                    return sur.iloc[0]['persona']\n",
    "                else: return None\n",
    "    return None #No person found with that surename with correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae9222-ba89-4d15-bfda-0edca423ff76",
   "metadata": {},
   "source": [
    "For each text record find preson with find_person function. It returns a id of person (or None). \n",
    "\n",
    "Check if predicted id is same as in text redord (saved in bool variable \"Correct\").\n",
    "\n",
    "Append result to a list of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccb213df-80d6-4cb0-a345-eace4ae8c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(row):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    import sys\n",
    "    sys.path.insert(0, 'Desktop/Unimi/information retriveal/project/corpus (Alfio)/inforet-project/')\n",
    "    try:\n",
    "        import os\n",
    "        os.chdir('Desktop/Unimi/information retriveal/project/corpus (Alfio)/inforet-project/')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    res = []\n",
    "    errors = []\n",
    "    \n",
    "    global regex\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "\n",
    "    global phase\n",
    "    try:\n",
    "        person = find_person(row['text'], row['date'], filtered_personnel)\n",
    "    except Exception as ex:\n",
    "        #errors.append({'index':index, 'text':row['text'], 'phase':phase, 'ex':ex})\n",
    "        errors.append({'text':row['text'], 'phase':phase, 'ex':ex})\n",
    "        return pd.DataFrame.from_dict(res), pd.DataFrame.from_dict(errors)\n",
    "        #continue\n",
    "    predicted = filtered_personnel.loc[filtered_personnel['persona'] == person]\n",
    "    if person is not None:\n",
    "        pred_surname = predicted.iloc[0]['surname']\n",
    "        pred_name = predicted.iloc[0]['name']\n",
    "    else:\n",
    "        pred_surname = None\n",
    "        pred_name = None\n",
    "    if row[\"persona\"] == person:\n",
    "        correct = True\n",
    "    else:\n",
    "        correct = False\n",
    "\n",
    "    res.append({\"True persona\": row[\"persona\"], \"True name\": row[\"name\"], \"True surname\": row[\"surname\"],\n",
    "                \"Text\":row[\"text\"],  \"Correct\":correct, \"Predicted persona\": person,\n",
    "                \"Predicted surname\": pred_surname, \n",
    "                \"Predicted name\": pred_name})\n",
    "\n",
    "    return pd.DataFrame.from_dict(res), pd.DataFrame.from_dict(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4f6f9c-5951-4f1d-9f13-419e4095a919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult(_push): pending>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc[:].push(dict(\n",
    "    words = words,\n",
    "    P = P,\n",
    "    correction = correction,\n",
    "    candidates = candidates,\n",
    "    known = known,\n",
    "    edits1 = edits1,\n",
    "    edits2 = edits2,\n",
    "    filtered_personnel=filtered_personnel,\n",
    "    find_person = find_person,\n",
    "    find_by_surname = find_by_surname,\n",
    "    find_by_surname_2 = find_by_surname_2,\n",
    "    find_by_name = find_by_name,\n",
    "    SURNAME = SURNAME,\n",
    "    NAME = NAME,\n",
    "    TEXT = TEXT,\n",
    "    WORDS = WORDS\n",
    "))\n",
    "view = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529e5749-d717-4098-8b02-8bd3738bbae0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    phase  \\\n",
      "0  VALENSISE RAFFAELE (gruppo MSI-destra nazional...  segment   \n",
      "\n",
      "                                  ex  \n",
      "0  int too large to convert to float  \n",
      "Done: 0/1000\n",
      "Done: 1/1000\n",
      "CPU times: total: 41.5 s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, df in enumerate(split_df):\n",
    "    ar = view.map_async(evaluation, [row for index, row in df.iterrows()])\n",
    "    ar.wait() # Wait until all tasks are done.\n",
    "    results = pd.DataFrame({'A' : []})\n",
    "    for res in ar:\n",
    "        #if len(res[1])>0:      #uncomment this line to print a list of errors\n",
    "            #print(res[1])      #uncomment this line to print a list of errors\n",
    "        results = pd.concat([results, res[0]], ignore_index=True)\n",
    "    results = results.drop(columns=['A'])\n",
    "    file_name = './result/res'+str(i+501)+'.csv'\n",
    "    results.to_csv(file_name,index=False)\n",
    "    clear_output(wait=True)     #comment this line to print a list of errors (otherwise this (printed) output will be cleared)\n",
    "    print('Done: ' + str(i) + '/' + str(len(split_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b78edd-dbb5-4d30-97cd-f2b1868379fd",
   "metadata": {},
   "source": [
    "# Forth method\n",
    "\n",
    "via nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4524735b-94f6-43c5-81fc-b0a806b0e469",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SURNAME done\n",
      "NAME done\n",
      "TEXT done\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def words(text): \n",
    "    text = text.upper().replace(\"À\",\"A'\").replace(\"È\",\"E'\").replace(\"É\",\"E'\").replace(\"Ì\",\"I'\").replace(\"Í\",\"I'\").replace(\"Ò\",\"O'\").replace(\"Ó\",\"O'\").replace(\"Ù\",\"U'\").replace(\"Ú\",\"U'\").replace(\"Ü\",\"U'\")\n",
    "    return re.findall(r\"[A-Z'-]+\", text.upper())\n",
    "SURNAME = Counter(words(' '.join(filtered_personnel['surname'])))\n",
    "print(\"SURNAME done\")\n",
    "NAME = Counter(words(' '.join(filtered_personnel['name'])))\n",
    "print(\"NAME done\")\n",
    "t = ' '.join(text['text'])\n",
    "\n",
    "TEXT = Counter(words(t))\n",
    "print(\"TEXT done\")\n",
    "WORDS = SURNAME\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb9103b-86a1-4aea-a470-cd35e5b818be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = \"abcdefghijklmnopqrstuvwxyz'\".upper()\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1da61-be17-4693-9dde-a7b3361f46b7",
   "metadata": {},
   "source": [
    "create one-gramms used in word-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276ac870-c252-45f2-89d1-d2367d7e264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"one-grams.txt\", 'w') as f:\n",
    "    c = TEXT+SURNAME+NAME\n",
    "    for k,v in  c.most_common():\n",
    "        f.write( \"{}\\t{}\\n\".format(k.lower(),v) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f41bdd-2f47-4ef6-9a0a-b67865e6c9a4",
   "metadata": {},
   "source": [
    "NER is made by find_person function.\n",
    "\n",
    "1) Transform text in acceptable form:\n",
    "\n",
    "    a) replace diacritics with \" ' \" symbol\n",
    "    \n",
    "    b) extract first substring before ',' of '.' symbols\n",
    "    \n",
    "    c) check if extracted substring sontains alphabetic words, if no, remove this substring and repeat (b) and (c)\n",
    "    \n",
    "2) Ceate list of persons based on a date of text\n",
    "3) Create list of entities found by nltk NER module and transform this list into a text\n",
    "4) Reduce list (2) with find_by_surname function by using text (3).\n",
    "5) If list (4) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "6) If list (4) have more than 1 id, reduce this list with find_by_name function.\n",
    "7) If list (6) contains only 1 preson (1 item), or list contains multiple items but all have same id, return id of first item in list.\n",
    "8) If list still have more than 1 id, or list is empty, return None (in paper was shown as 'no person found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f214dd0b-3585-4b11-b287-b9b40e939f1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_by_surname(text,personnel, corr):\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    data = []\n",
    "    for i, s in personnel.iterrows():\n",
    "        if re.search(r\"\\b\" + re.escape(s['surname']) + r\"\\b\", text):\n",
    "            data.append(s)\n",
    "    sur = pd.DataFrame(data)\n",
    "    if len(sur.index) == 0 and not corr:\n",
    "        WORDS = SURNAME+TEXT\n",
    "        word = regex.sub('', text).split()\n",
    "        for i, w in enumerate(word):\n",
    "            word[i] = correction(w)\n",
    "        sur = find_by_surname(' '.join(word),personnel,True)\n",
    "    return sur\n",
    "\n",
    "def find_by_name(text,personnel,corr):\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    for i, s in personnel.iterrows():\n",
    "        if s['name'] in text:\n",
    "            data.append(s)\n",
    "    sur = pd.DataFrame(data)\n",
    "    return sur\n",
    "\n",
    "def find_person(text, date, personnel):\n",
    "    global phase\n",
    "    import segment\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import string\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    sw = stopwords.words('italian')\n",
    "    global l\n",
    "    phase = \"char_replace\"\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "    text = text.replace(\"À\",\"A'\")\n",
    "    text = text.replace(\"È\",\"E'\")\n",
    "    text = text.replace(\"É\",\"E'\")\n",
    "    text = text.replace(\"Ì\",\"I'\")\n",
    "    text = text.replace(\"Ò\",\"O'\")\n",
    "    text = text.replace(\"Ó\",\"O'\")\n",
    "    text = text.replace(\"Ù\",\"U'\")\n",
    "    sentences = re.split('[,.]', text)\n",
    "    while(not sentences==[]):\n",
    "        sentences[0] = regex.sub('', sentences[0])\n",
    "        if (re.match(\"^[a-zA-Z\\s']+$\", sentences[0])):\n",
    "            if sentences[0].isspace():\n",
    "                sentences.pop(0)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            sentences.pop(0)\n",
    "    if not sentences==[]:\n",
    "        text = sentences[0].lstrip()\n",
    "\n",
    "    #print(text)\n",
    "    # take only up to 20 first words\n",
    "    word = regex.sub('', text).split()\n",
    "    #print(word)\n",
    "    average = sum(len(w) for w in word) / len(word)\n",
    "    if average < 4:\n",
    "        if len(word) >= 120: #avarage 6-8 letters in italian word (6*20 = 120), but segment have limit of 100\n",
    "            seg = segment.segment(''.join(word[0:60]))\n",
    "        else:\n",
    "            seg = segment.segment(''.join(word))\n",
    "        text = ' '.join(seg)\n",
    "    elif len(word) >= 20:\n",
    "        text = ' '.join(word[0:20])\n",
    "    #print('Text:')\n",
    "    #print(text)\n",
    "    #print('====================')\n",
    "    phase = \"date_search\"\n",
    "    date = personnel[(personnel[\"dataInizio\"] <= date) & (personnel[\"dataFine\"] >= date)]\n",
    "    chunk_list = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary=True): # when binary=True Surnames may be seen as GPE or ORGANIZATION\n",
    "            if hasattr(chunk, 'label'):\n",
    "                chunk_list.append(' '.join(c[0] for c in chunk))\n",
    "                #print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    chunked_name = ' '.join(chunk_list)\n",
    "    #print(chunked_name)\n",
    "    #print('====================')\n",
    "    if len(date.index) >= 1:\n",
    "        phase = \"sur_search\"\n",
    "        sur = find_by_surname(chunked_name.upper(), date, False)\n",
    "        #print(sur)\n",
    "        if len(sur.index) == 0:\n",
    "            return None\n",
    "        elif (len(sur.index) == 1) or (len(pd.unique(sur['persona'])) == 1):\n",
    "            return sur.iloc[0]['persona']\n",
    "        else:\n",
    "            if len(sur['surname'].value_counts()) > 1:\n",
    "                word = regex.sub('', text.upper()).split()\n",
    "                sur2 = pd.DataFrame({'A' : []})\n",
    "                i = 0\n",
    "                t = ''\n",
    "                while(sur2.empty and (i < 20 and i < len(word))): #limit of frase up to 20 words\n",
    "                    t = t + ' ' + word[i]\n",
    "                    sur2 = find_by_surname(t,sur,False) # check what surname is appering first in text \n",
    "                    i+=1\n",
    "                sur = sur2\n",
    "                if sur.empty:\n",
    "                    return None\n",
    "            #print(sur)\n",
    "            if len(pd.unique(sur['persona'])) == 1:\n",
    "                return sur.iloc[0]['persona']\n",
    "            else:\n",
    "                phase = \"name_search\"\n",
    "                name = find_by_name(text.upper(), sur, False)\n",
    "                #print('-------------------')\n",
    "                #print(name)\n",
    "                if len(name.index) == 1 or (len(name.index) > 1 and len(pd.unique(name['persona']))==1):\n",
    "                    return name.iloc[0]['persona']\n",
    "    else:#No person found with that surename with correction\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bacfd8-bc4f-4d11-9aa0-6e2810f1dd93",
   "metadata": {},
   "source": [
    "For each text record find preson with find_person function. It returns a id of person (or None). \n",
    "\n",
    "Check if predicted id is same as in text redord (saved in bool variable \"Correct\").\n",
    "\n",
    "Append result to a list of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53157999-5937-4b33-a9cb-ae594849c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(row):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    import sys\n",
    "    sys.path.insert(0, 'Desktop/Unimi/information retriveal/project/corpus (Alfio)/inforet-project/')\n",
    "    try:\n",
    "        import os\n",
    "        os.chdir('Desktop/Unimi/information retriveal/project/corpus (Alfio)/inforet-project/')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    res = []\n",
    "    errors = []\n",
    "    \n",
    "    global regex\n",
    "    regex = re.compile(\"[^a-zA-Z -']\")\n",
    "\n",
    "    global phase\n",
    "    try:\n",
    "        person = find_person(row['text'], row['date'], filtered_personnel)\n",
    "    except Exception as ex:\n",
    "        #errors.append({'index':index, 'text':row['text'], 'phase':phase, 'ex':ex})\n",
    "        errors.append({'text':row['text'], 'phase':phase, 'ex':ex})\n",
    "        return pd.DataFrame.from_dict(res), pd.DataFrame.from_dict(errors)\n",
    "        #continue\n",
    "    predicted = filtered_personnel.loc[filtered_personnel['persona'] == person]\n",
    "    if person is not None:\n",
    "        pred_surname = predicted.iloc[0]['surname']\n",
    "        pred_name = predicted.iloc[0]['name']\n",
    "    else:\n",
    "        pred_surname = None\n",
    "        pred_name = None\n",
    "    if row[\"persona\"] == person:\n",
    "        correct = True\n",
    "    else:\n",
    "        correct = False\n",
    "\n",
    "    res.append({\"True persona\": row[\"persona\"], \"True name\": row[\"name\"], \"True surname\": row[\"surname\"],\n",
    "                \"Text\":row[\"text\"],  \"Correct\":correct, \"Predicted persona\": person,\n",
    "                \"Predicted surname\": pred_surname, \n",
    "                \"Predicted name\": pred_name})\n",
    "\n",
    "    return pd.DataFrame.from_dict(res), pd.DataFrame.from_dict(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4305cf9d-ed22-42af-8665-008329211650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc[:].push(dict(\n",
    "    words = words,\n",
    "    P = P,\n",
    "    correction = correction,\n",
    "    candidates = candidates,\n",
    "    known = known,\n",
    "    edits1 = edits1,\n",
    "    edits2 = edits2,\n",
    "    filtered_personnel=filtered_personnel,\n",
    "    find_person = find_person,\n",
    "    find_by_surname = find_by_surname,\n",
    "    find_by_name = find_by_name,\n",
    "    SURNAME = SURNAME,\n",
    "    NAME = NAME,\n",
    "    TEXT = TEXT,\n",
    "    WORDS = WORDS\n",
    "))\n",
    "view = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5634bcea-2c3f-40ec-bdc1-5be6f5b3900a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0/1000\n",
      "Done: 1/1000\n",
      "CPU times: total: 43.5 s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, df in enumerate(split_df):\n",
    "    ar = view.map_async(evaluation, [row for index, row in df.iterrows()])\n",
    "    ar.wait() # Wait until all tasks are done.\n",
    "    results = pd.DataFrame({'A' : []})\n",
    "    for res in ar:\n",
    "        #if len(res[1])>0:      #uncomment this line to print a list of errors\n",
    "            #print(res[1])      #uncomment this line to print a list of errors\n",
    "        results = pd.concat([results, res[0]], ignore_index=True)\n",
    "    results = results.drop(columns=['A'])\n",
    "    file_name = './result/res'+str(i+501)+'.csv'\n",
    "    results.to_csv(file_name,index=False)\n",
    "    clear_output(wait=True)     #comment this line to print a list of errors (otherwise this (printed) output will be cleared)\n",
    "    print('Done: ' + str(i) + '/' + str(len(split_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac7615-b6fc-45c9-a5fc-24f04d0ecda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
